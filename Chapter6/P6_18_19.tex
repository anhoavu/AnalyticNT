\unless\ifdefined\IsMainDocument
\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Fhat}{\widehat{F}}
\newcommand{\Abs}[1]{\left| #1 \right|}
\begin{document}
\fi

\textbf{Problem 6.18}: If the Dirichlet series $\sum c_n n^{-s}$ converges at $s = \sigma_0 + i t_0$, prove that the function defined by the series for $\Re s > \sigma_0$ cannot have a pole on the line $\Re s = \sigma_0$.

\begin{proof}
Let $F(x) = \sum_{n \leq x} c_n$. Without loss of generality, we could assume $s_0 = 0$.

In other words, it suffices to show that if $\sum c_n = \lim_{x \rightarrow \infty} F(x)$ converges then the function defined by $\sum c_n n^{-s}$ on the right half plane $H := \{s : \Re s > 0\}$ cannot have a pole on the imaginary axis.

If we manage to do that, the general statement follows by applying this simplification with $c_n n^{-(\sigma_0 + i t_0)}$ in place of the $c_n$. The technique of translating the argument $s$ by $s - (\sigma_0 + i t_0)$ is used the proof of Theorem 6.9 and 6.15.

Now let $c = F(\infty) = \sum c_n$ and assume that $\Fhat(s)$ has a pole of order $k$ at some point on the imaginary axis, say at $i t \not= 0$. Then the limit
$$\lim_{s \rightarrow i t} (s - i t)^k \Fhat(s), \qquad s \in H$$
exists and is non-zero. In this problem, we only need to care about the simpler limit on the horizontal line $\Im s = t$, namely
$$\lim_{\sigma \rightarrow 0^+} \sigma^k \Fhat(\sigma + i t)  \text{ exists and } \not= 0.$$

Using equation (6.5) in the proof\footnote{We are in the case $\alpha = 0$ i.e. $F(x) - c = O(1)$ there. Since $F(x) \rightarrow c$, for $X$ large enough $|F(x) - c| < 1$ for all $x \geq X$. Then we have an upper bound $|F(x) - c| \leq 1 + \sum_{n \leq X} |F(n) - c|$ for all $x$.} of Theorem 6.9
$$\Fhat(s) = c + s \int_1^\infty x^{-s-1} (F(x) - c) dx, \qquad \forall s \in H,$$
we obtain
\begin{align*}
\lim_{\sigma \rightarrow 0^+} \sigma^k \Fhat(\sigma + i t)
&= \lim_{\sigma \rightarrow 0^+} c \sigma^k + \sigma^k (\sigma + i t) \int_1^\infty x^{-\sigma - i t - 1} (F(x) - c) dx\\
&= (it) \lim_{\sigma \rightarrow 0^+} \underbrace{\sigma^k \int_1^\infty x^{-\sigma - i t - 1} (F(x) - c) dx}_{H_\infty(\sigma)}
\end{align*}
where for $X \in [1, \infty]$, we define the functions
$$H_X(\sigma) = \sigma^k \int_1^X x^{-\sigma - i t -1} (F(x) - c) dx$$
on $(0, 1)$.

We claim that $H_X$ converges uniformly to $H_\infty$ as $X \rightarrow \infty$: Let $\epsilon > 0$ be arbitrary and simply pick $X > 1$ such that $|F(x) - c| < \epsilon$ for all $x > X$. Then for any $Y > X$, we find $\Abs{ H_Y(\sigma) - H_\infty(\sigma) } < \epsilon$ for all $\sigma \in \R^+$ because
\begin{align*}
\Abs{ H_Y(\sigma) - H_\infty(\sigma) } &= \Abs{ \sigma^k \int_Y^\infty x^{-\sigma - i t -1} (F(x) - c) dx } \\
&\leq \sigma^k \int_Y^\infty x^{-\sigma-1} |F(x) - c| dx \\
&\leq \epsilon \sigma^k \int_Y^\infty x^{-\sigma-1} dx \\
&= \epsilon \sigma^{k-1} Y^{-\sigma} \\
&\leq \epsilon
\end{align*}
as long as $\sigma \in (0, 1)$. Here since $Y > X > 1$, the exponential function $t \mapsto Y^t$ is increasing on $\R$ and so $Y^{-\sigma} < Y^0 = 1$.

Uniform convergence allows us to swap the two limits over $\sigma$ and $X$ and get a contradiction
\begin{align*}
0 \not= \lim_{\sigma \rightarrow 0^+} H_\infty(\sigma) &= \lim_{\sigma \rightarrow 0^+} \lim_{X \rightarrow \infty} H_X(\sigma) \\
&= \lim_{X \rightarrow \infty} \lim_{\sigma \rightarrow 0^+} H_X(\sigma) \\
&= \lim_{X \rightarrow \infty} \lim_{\sigma \rightarrow 0^+} \sigma^k \int_1^X x^{-\sigma - i t -1} (F(x) - c) dx \\
&= \lim_{X \rightarrow \infty} \lim_{\sigma \rightarrow 0^+} \sigma^k \left(\left.\frac{(F(x) - c)x^{-\sigma-it}}{-\sigma - it}\right|_1^X - \sum_{1 < n \leq X} \frac{c_n n^{-\sigma - i t}}{-\sigma - it} \right) \\
&= 0
\end{align*}
since the expression in parentheses converges as $\sigma \rightarrow 0^+$.

\textbf{Remark}: The previous problem showed that this property only works for Dirichlet series. The Mellin transform there has two simple poles on the line $\Re s = 1$ and converges otherwise.
\end{proof}

\textbf{Problem 6.19}: Use the preceding problem, show that
$$\sum_{n=1}^{\infty} \mu(n) n^{-\frac12 - it} \qquad \text{ and } \qquad \sum_{n=1}^{\infty} n^{-1 - it}$$
each diverge for all real $t$. (You may assume that the Riemann zeta function has zeros with real part $1/2$.)

\begin{proof}
Let $\eta(s) = \sum \mu(n) n^{-s}$ whenever the limit exists and assume $\eta(-\frac12 - it)$ converges for some real $t$. Then $\eta(s)$ cannot have a pole on $\Re s = 1/2$. We also have $\eta(s)$ converges for $\Re s > 1/2$ which implies analytic continuation of $\zeta(s)$ to the half plane $\Re s > 1/2$ with the the places where $\eta = 0$ removed. We have the identity
$$\zeta(s) \eta(s) = 1$$
on $\Re s > 1$. This identity still holds for the extended $\zeta$ function by identity theorem in complex analysis (view both sides as complex analytic functions). But then $\eta(s)$ has some pole on the line $\Re s = 1/2$ because $\zeta(s)$ has zeros on that line. We thus reached a contradiction. So $\eta(s)$ does not converge.

The second one is obvious: If $\sum_{n=1}^{\infty} n^{-1 - it}$ converges for some real $t$ then the function $\zeta(s) = \sum n^{-s}$ cannot have a pole on the line $\Re s = 1$. But $\zeta(s)$ does have a pole at $s = 1$; a contradiction. So $\sum_{n=1}^{\infty} n^{-1 - it}$ diverges for all real $t$.
\end{proof}

\unless\ifdefined\IsMainDocument
\end{document}
\fi
