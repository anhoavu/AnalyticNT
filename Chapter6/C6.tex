\unless\ifdefined\IsMainDocument
\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb,hyperref}
\newcommand{\Fhat}{\widehat{F}}
\newcommand{\Abs}[1]{\left| #1 \right|}
\begin{document}
\fi

\section{A partial converse to Lemma 6.4: Euler product implies convergence of series}

In Lemma 6.4, we show the Euler product formula
$$\sum f(n) = \prod (1 + f(p) + f(p^2) + \cdots)$$
assuming the series converges absolutely.

Let me prove a simple partial converse here: Suppose that $f \geq 0$ is multiplicative and that the infinite product $\prod (1 + f(p) + f(p^2) + \cdots)$ converges. Then the series $\sum f(n)$ converges absolutely (and hence, the Euler product above holds by the original Lemma 6.4).

By assumption $f \geq 0$, the sequence of partial sums for the series $\sum_{n \leq N} f(n)$ is an increasing sequence so to establish convergence, it suffices to show that it is bounded above and an obvious candidate for such bound is the limit of the infinite product.

To prove that, let $C_N$ be the finite set of all positive integers $n$ such that (i) every prime divisor of $n$ does not exceed $N$; and (ii) if $p^k | n$ then $k \leq N$. In other words, if $p_1, ..., p_r$ is the list of all primes $\leq N$ (i.e. $r = \pi(N)$) then
$$C_N = \{p_1^{\alpha_1} p_2^{\alpha_2} \cdots p_r^{\alpha_r} : 0 \leq \alpha_i \leq N \text{ for all } i\}.$$
It is clear that $C_N \supseteq \{1, 2, ..., N\}$. Therefore,
\begin{align*}
\sum_{n \leq N} f(n) &\leq \sum_{n \in C_N} f(n) \\
&= \prod_{p \leq N} (1 + f(p) + \cdots + f(p^N)) \\
&< \prod_{p \leq N} (1 + f(p) + f(p^2) + \cdots) \\
&< \prod_p (1 + f(p) + f(p^2) + \cdots).
\end{align*}
And so the series $\sum f(n)$ converges.

\section{On the simple logarithm bound in Example 6.8}

In Example 6.8, we have the simple bound
$$0 > \log\left(1 - \frac{1}{K}\right) > \frac{-1}{K - 1}, \qquad K > 2.$$

This inequality actually applies to all $K > 1$. Let me rewrite it as
$$0 > \log(1 - x) > -\frac{1}{x^{-1} - 1} = -\frac{x}{1 - x}, \qquad x \in (0, 1).$$

To prove that, recall the classical inequality
$$\log(1 + x) \leq x, \qquad \text{ for } x \geq -1$$
which could be proved by considering the function
$$f(x) = x - \log(1 + x)$$
defined on $(-1, \infty)$. Its derivative
$$f'(x) = 1 - \frac{1}{1 + x} = \frac{x}{1 + x}$$
is positive if $x > 0$ and is negative if $-1 < x < 0$. So $f(x) \geq f(0) = 0$ for all $x \in (-1, \infty)$.

Now back to our claimed inequality: We have
$$-\log(1 - x) = \log\left(\frac{1}{1 - x}\right) = \log\left(1 + \frac{x}{1 -  x}\right) \leq \frac{x}{1 - x}$$
as long as $\frac{x}{1 - x} \geq -1$ which is obvious if $x \in (0, 1)$.

\section{Uniform Convergence}

When we say $\Fhat$ converges uniformly on $U$, what we mean is: For any $\epsilon > 0$, there exists an $X_0$ such that for any $X > X_0$,
$$\Abs{\int_{1^-}^X x^{-s} dF(x) - \Fhat(s)} < \epsilon$$
for all $s \in U$.

In other words, the uniform convergence is for the $\lim_{X \rightarrow \infty}$ in the definition of $\Fhat$.

To see how this goes back to the \href{https://en.wikipedia.org/wiki/Uniform\_convergence}{classical definition of uniform convergence} of a sequence of functions $f_n \rightarrow f$, the ``sequence'' of functions in this case is
$$\Fhat_X(s) = \int_{1^-}^X x^{-s} dF(x).$$
So $\Fhat_X \rightarrow \Fhat$ uniformly, by the classical definition, means that for any $\epsilon > 0$, we can find a single $X_0(\epsilon)$ i.e. independent of $s$ such that
$$\Abs{ \Fhat_X(s) - \Fhat(s) } < \epsilon$$
as long as $X > X_0(\epsilon)$. See also the proof of Corollary 6.16.

Due to completeness of the real/complex numbers, the above definition has an equivalent Cauchy's version: $\Fhat_X \rightarrow \Fhat$ uniformly if and only if for any $\epsilon > 0$, we can find a single $X_0(\epsilon)$ such that for all $Y, Z > X_0(\epsilon)$, we have
$$\Abs{ \Fhat_Y(s) - \Fhat_Z(s) } < \epsilon.$$
This is the condition we used in the proof of Theorem 6.15. Also note in the proof of that Theorem, the reason we can write
$$\int_Y^Z x^{-s} dF(x) = \int_Y^Z x^{-(s-s_0)} d\psi(x)$$
is to first express
$$\int_Y^Z x^{-s} dF(x) = \int_Y^Z x^{-s_0} x^{-(s-s_0)} dF(x) = \int_Y^Z x^{-s_0} d\psi_0(x)$$
where 
$$\psi_0(y) = \int_{1^-}^y x^{-(s-s_0)} dF(x).$$
And then we observe that $\psi_0(y) - \psi(y) = K$ is a constant, namely $K = \int_{1^-}^{\infty} x^{-s_0} dF(x)$ by convergence assumption. This implies $\psi(y) = \psi_0(y) - K$ and so $d\psi = d\psi_0$.

\section{Weirstrass M test}

The classical statement is: Let $f_n$ be a sequence of functions $f_n: E \to \mathbb {C}$ and let $M_n$ be a sequence of positive real numbers such that $|f_n(x)|\leq M_{n}$ for all $x \in E$ and $n = 1, 2, 3, \ldots$. If $\sum M_n$ converges, then $\sum f_n$ converges uniformly on $E$.

In the proof of Lemma 6.13, we are using the obvious inequality
$$\Abs{ \int_Y^Z x^{-s} dF(x) } \leq \int_Y^Z x^{-b} dF_v$$
for all $s \in \{\sigma \geq b\}$ and the assumption $\int x^{-b} dF_v < \infty$ to deduce $\int x^{-s} dF(x)$ converges uniformly on that half plane.

\section{A consequence of Example 6.29}

We proved here that $\zeta(s) = \exp\left(\sum \kappa(n) n^{-s}\right)$ on $\{\Re s > 1\}$. This implies that $\zeta(s)$ has no zero on that halfplane.

\section{The inequality in the second proof of Lemma 6.30}

The inequality $n^\alpha \leq n^{\alpha + 1} - n^{\alpha}$ can be shown by noting that
$$n^\alpha \leq n^{\alpha + 1} - (n - 1)^{\alpha + 1} \iff n^\alpha + (n-1)^{\alpha + 1} \leq n^{\alpha + 1}$$
which easily follows from the trivial bound
$$(n-1)^{\alpha + 1} = (n-1) (n-1)^{\alpha} \leq (n-1) n^{\alpha}$$
whence
$$n^\alpha + (n-1)^{\alpha + 1} \leq n^{\alpha} + (n-1) n^{\alpha} = n^{\alpha + 1}.$$

\unless\ifdefined\IsMainDocument
\end{document}
\fi
